{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc8b4d94",
   "metadata": {},
   "source": [
    "# Simple Binary Classification\n",
    "\n",
    "The purpose of this exercise is to study the binary classification problem, the idea is to use different methods to classify a given dataset.This time we are going to study the Sonar dataset. \n",
    "\n",
    "The Sonar Dataset involves the prediction of whether or not an object is a mine or a rock given the strength of sonar returns at different angles. It is a binary (2-class) classification problem. The number of observations for each class is not balanced. There are 208 observations with 60 input variables and 1 output variable. \n",
    "\n",
    "The CSV file contains 111 records of sonar signals dating over mines from different angles, it also contains 97 records of sonar signals dating over rocks under the same mine conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39b562ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data\n",
    "using CSV\n",
    "using DataFrames \n",
    "using StatsBase\n",
    "using Statistics\n",
    "\n",
    "df = DataFrame(CSV.File(\"C:/Users/maria/Desktop/Universidad/2022-I/Matemáticas para ML/Databases/sonar.csv\"));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba92ce5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>Y</th><th>Y_1</th><th>Y_2</th><th>Y_3</th><th>Y_4</th><th>Y_5</th><th>Y_6</th><th>Y_7</th><th>Y_8</th></tr><tr><th></th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th></tr></thead><tbody><p>207 rows × 61 columns (omitted printing of 52 columns)</p><tr><th>1</th><td>0.0453</td><td>0.0523</td><td>0.0843</td><td>0.0689</td><td>0.1183</td><td>0.2583</td><td>0.2156</td><td>0.3481</td><td>0.3337</td></tr><tr><th>2</th><td>0.0262</td><td>0.0582</td><td>0.1099</td><td>0.1083</td><td>0.0974</td><td>0.228</td><td>0.2431</td><td>0.3771</td><td>0.5598</td></tr><tr><th>3</th><td>0.01</td><td>0.0171</td><td>0.0623</td><td>0.0205</td><td>0.0205</td><td>0.0368</td><td>0.1098</td><td>0.1276</td><td>0.0598</td></tr><tr><th>4</th><td>0.0762</td><td>0.0666</td><td>0.0481</td><td>0.0394</td><td>0.059</td><td>0.0649</td><td>0.1209</td><td>0.2467</td><td>0.3564</td></tr><tr><th>5</th><td>0.0286</td><td>0.0453</td><td>0.0277</td><td>0.0174</td><td>0.0384</td><td>0.099</td><td>0.1201</td><td>0.1833</td><td>0.2105</td></tr><tr><th>6</th><td>0.0317</td><td>0.0956</td><td>0.1321</td><td>0.1408</td><td>0.1674</td><td>0.171</td><td>0.0731</td><td>0.1401</td><td>0.2083</td></tr><tr><th>7</th><td>0.0519</td><td>0.0548</td><td>0.0842</td><td>0.0319</td><td>0.1158</td><td>0.0922</td><td>0.1027</td><td>0.0613</td><td>0.1465</td></tr><tr><th>8</th><td>0.0223</td><td>0.0375</td><td>0.0484</td><td>0.0475</td><td>0.0647</td><td>0.0591</td><td>0.0753</td><td>0.0098</td><td>0.0684</td></tr><tr><th>9</th><td>0.0164</td><td>0.0173</td><td>0.0347</td><td>0.007</td><td>0.0187</td><td>0.0671</td><td>0.1056</td><td>0.0697</td><td>0.0962</td></tr><tr><th>10</th><td>0.0039</td><td>0.0063</td><td>0.0152</td><td>0.0336</td><td>0.031</td><td>0.0284</td><td>0.0396</td><td>0.0272</td><td>0.0323</td></tr><tr><th>11</th><td>0.0123</td><td>0.0309</td><td>0.0169</td><td>0.0313</td><td>0.0358</td><td>0.0102</td><td>0.0182</td><td>0.0579</td><td>0.1122</td></tr><tr><th>12</th><td>0.0079</td><td>0.0086</td><td>0.0055</td><td>0.025</td><td>0.0344</td><td>0.0546</td><td>0.0528</td><td>0.0958</td><td>0.1009</td></tr><tr><th>13</th><td>0.009</td><td>0.0062</td><td>0.0253</td><td>0.0489</td><td>0.1197</td><td>0.1589</td><td>0.1392</td><td>0.0987</td><td>0.0955</td></tr><tr><th>14</th><td>0.0124</td><td>0.0433</td><td>0.0604</td><td>0.0449</td><td>0.0597</td><td>0.0355</td><td>0.0531</td><td>0.0343</td><td>0.1052</td></tr><tr><th>15</th><td>0.0298</td><td>0.0615</td><td>0.065</td><td>0.0921</td><td>0.1615</td><td>0.2294</td><td>0.2176</td><td>0.2033</td><td>0.1459</td></tr><tr><th>16</th><td>0.0352</td><td>0.0116</td><td>0.0191</td><td>0.0469</td><td>0.0737</td><td>0.1185</td><td>0.1683</td><td>0.1541</td><td>0.1466</td></tr><tr><th>17</th><td>0.0192</td><td>0.0607</td><td>0.0378</td><td>0.0774</td><td>0.1388</td><td>0.0809</td><td>0.0568</td><td>0.0219</td><td>0.1037</td></tr><tr><th>18</th><td>0.027</td><td>0.0092</td><td>0.0145</td><td>0.0278</td><td>0.0412</td><td>0.0757</td><td>0.1026</td><td>0.1138</td><td>0.0794</td></tr><tr><th>19</th><td>0.0126</td><td>0.0149</td><td>0.0641</td><td>0.1732</td><td>0.2565</td><td>0.2559</td><td>0.2947</td><td>0.411</td><td>0.4983</td></tr><tr><th>20</th><td>0.0473</td><td>0.0509</td><td>0.0819</td><td>0.1252</td><td>0.1783</td><td>0.307</td><td>0.3008</td><td>0.2362</td><td>0.383</td></tr><tr><th>21</th><td>0.0664</td><td>0.0575</td><td>0.0842</td><td>0.0372</td><td>0.0458</td><td>0.0771</td><td>0.0771</td><td>0.113</td><td>0.2353</td></tr><tr><th>22</th><td>0.0099</td><td>0.0484</td><td>0.0299</td><td>0.0297</td><td>0.0652</td><td>0.1077</td><td>0.2363</td><td>0.2385</td><td>0.0075</td></tr><tr><th>23</th><td>0.0115</td><td>0.015</td><td>0.0136</td><td>0.0076</td><td>0.0211</td><td>0.1058</td><td>0.1023</td><td>0.044</td><td>0.0931</td></tr><tr><th>24</th><td>0.0293</td><td>0.0644</td><td>0.039</td><td>0.0173</td><td>0.0476</td><td>0.0816</td><td>0.0993</td><td>0.0315</td><td>0.0736</td></tr><tr><th>25</th><td>0.0201</td><td>0.0026</td><td>0.0138</td><td>0.0062</td><td>0.0133</td><td>0.0151</td><td>0.0541</td><td>0.021</td><td>0.0505</td></tr><tr><th>26</th><td>0.0151</td><td>0.032</td><td>0.0599</td><td>0.105</td><td>0.1163</td><td>0.1734</td><td>0.1679</td><td>0.1119</td><td>0.0889</td></tr><tr><th>27</th><td>0.0177</td><td>0.03</td><td>0.0288</td><td>0.0394</td><td>0.063</td><td>0.0526</td><td>0.0688</td><td>0.0633</td><td>0.0624</td></tr><tr><th>28</th><td>0.01</td><td>0.0275</td><td>0.019</td><td>0.0371</td><td>0.0416</td><td>0.0201</td><td>0.0314</td><td>0.0651</td><td>0.1896</td></tr><tr><th>29</th><td>0.0189</td><td>0.0308</td><td>0.0197</td><td>0.0622</td><td>0.008</td><td>0.0789</td><td>0.144</td><td>0.1451</td><td>0.1789</td></tr><tr><th>30</th><td>0.024</td><td>0.0218</td><td>0.0324</td><td>0.0569</td><td>0.033</td><td>0.0513</td><td>0.0897</td><td>0.0713</td><td>0.0569</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccccccc}\n",
       "\t& Y & Y\\_1 & Y\\_2 & Y\\_3 & Y\\_4 & Y\\_5 & Y\\_6 & Y\\_7 & Y\\_8 & \\\\\n",
       "\t\\hline\n",
       "\t& Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & \\\\\n",
       "\t\\hline\n",
       "\t1 & 0.0453 & 0.0523 & 0.0843 & 0.0689 & 0.1183 & 0.2583 & 0.2156 & 0.3481 & 0.3337 & $\\dots$ \\\\\n",
       "\t2 & 0.0262 & 0.0582 & 0.1099 & 0.1083 & 0.0974 & 0.228 & 0.2431 & 0.3771 & 0.5598 & $\\dots$ \\\\\n",
       "\t3 & 0.01 & 0.0171 & 0.0623 & 0.0205 & 0.0205 & 0.0368 & 0.1098 & 0.1276 & 0.0598 & $\\dots$ \\\\\n",
       "\t4 & 0.0762 & 0.0666 & 0.0481 & 0.0394 & 0.059 & 0.0649 & 0.1209 & 0.2467 & 0.3564 & $\\dots$ \\\\\n",
       "\t5 & 0.0286 & 0.0453 & 0.0277 & 0.0174 & 0.0384 & 0.099 & 0.1201 & 0.1833 & 0.2105 & $\\dots$ \\\\\n",
       "\t6 & 0.0317 & 0.0956 & 0.1321 & 0.1408 & 0.1674 & 0.171 & 0.0731 & 0.1401 & 0.2083 & $\\dots$ \\\\\n",
       "\t7 & 0.0519 & 0.0548 & 0.0842 & 0.0319 & 0.1158 & 0.0922 & 0.1027 & 0.0613 & 0.1465 & $\\dots$ \\\\\n",
       "\t8 & 0.0223 & 0.0375 & 0.0484 & 0.0475 & 0.0647 & 0.0591 & 0.0753 & 0.0098 & 0.0684 & $\\dots$ \\\\\n",
       "\t9 & 0.0164 & 0.0173 & 0.0347 & 0.007 & 0.0187 & 0.0671 & 0.1056 & 0.0697 & 0.0962 & $\\dots$ \\\\\n",
       "\t10 & 0.0039 & 0.0063 & 0.0152 & 0.0336 & 0.031 & 0.0284 & 0.0396 & 0.0272 & 0.0323 & $\\dots$ \\\\\n",
       "\t11 & 0.0123 & 0.0309 & 0.0169 & 0.0313 & 0.0358 & 0.0102 & 0.0182 & 0.0579 & 0.1122 & $\\dots$ \\\\\n",
       "\t12 & 0.0079 & 0.0086 & 0.0055 & 0.025 & 0.0344 & 0.0546 & 0.0528 & 0.0958 & 0.1009 & $\\dots$ \\\\\n",
       "\t13 & 0.009 & 0.0062 & 0.0253 & 0.0489 & 0.1197 & 0.1589 & 0.1392 & 0.0987 & 0.0955 & $\\dots$ \\\\\n",
       "\t14 & 0.0124 & 0.0433 & 0.0604 & 0.0449 & 0.0597 & 0.0355 & 0.0531 & 0.0343 & 0.1052 & $\\dots$ \\\\\n",
       "\t15 & 0.0298 & 0.0615 & 0.065 & 0.0921 & 0.1615 & 0.2294 & 0.2176 & 0.2033 & 0.1459 & $\\dots$ \\\\\n",
       "\t16 & 0.0352 & 0.0116 & 0.0191 & 0.0469 & 0.0737 & 0.1185 & 0.1683 & 0.1541 & 0.1466 & $\\dots$ \\\\\n",
       "\t17 & 0.0192 & 0.0607 & 0.0378 & 0.0774 & 0.1388 & 0.0809 & 0.0568 & 0.0219 & 0.1037 & $\\dots$ \\\\\n",
       "\t18 & 0.027 & 0.0092 & 0.0145 & 0.0278 & 0.0412 & 0.0757 & 0.1026 & 0.1138 & 0.0794 & $\\dots$ \\\\\n",
       "\t19 & 0.0126 & 0.0149 & 0.0641 & 0.1732 & 0.2565 & 0.2559 & 0.2947 & 0.411 & 0.4983 & $\\dots$ \\\\\n",
       "\t20 & 0.0473 & 0.0509 & 0.0819 & 0.1252 & 0.1783 & 0.307 & 0.3008 & 0.2362 & 0.383 & $\\dots$ \\\\\n",
       "\t21 & 0.0664 & 0.0575 & 0.0842 & 0.0372 & 0.0458 & 0.0771 & 0.0771 & 0.113 & 0.2353 & $\\dots$ \\\\\n",
       "\t22 & 0.0099 & 0.0484 & 0.0299 & 0.0297 & 0.0652 & 0.1077 & 0.2363 & 0.2385 & 0.0075 & $\\dots$ \\\\\n",
       "\t23 & 0.0115 & 0.015 & 0.0136 & 0.0076 & 0.0211 & 0.1058 & 0.1023 & 0.044 & 0.0931 & $\\dots$ \\\\\n",
       "\t24 & 0.0293 & 0.0644 & 0.039 & 0.0173 & 0.0476 & 0.0816 & 0.0993 & 0.0315 & 0.0736 & $\\dots$ \\\\\n",
       "\t25 & 0.0201 & 0.0026 & 0.0138 & 0.0062 & 0.0133 & 0.0151 & 0.0541 & 0.021 & 0.0505 & $\\dots$ \\\\\n",
       "\t26 & 0.0151 & 0.032 & 0.0599 & 0.105 & 0.1163 & 0.1734 & 0.1679 & 0.1119 & 0.0889 & $\\dots$ \\\\\n",
       "\t27 & 0.0177 & 0.03 & 0.0288 & 0.0394 & 0.063 & 0.0526 & 0.0688 & 0.0633 & 0.0624 & $\\dots$ \\\\\n",
       "\t28 & 0.01 & 0.0275 & 0.019 & 0.0371 & 0.0416 & 0.0201 & 0.0314 & 0.0651 & 0.1896 & $\\dots$ \\\\\n",
       "\t29 & 0.0189 & 0.0308 & 0.0197 & 0.0622 & 0.008 & 0.0789 & 0.144 & 0.1451 & 0.1789 & $\\dots$ \\\\\n",
       "\t30 & 0.024 & 0.0218 & 0.0324 & 0.0569 & 0.033 & 0.0513 & 0.0897 & 0.0713 & 0.0569 & $\\dots$ \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ &  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "207×61 DataFrame. Omitted printing of 54 columns\n",
       "│ Row │ Y       │ Y_1     │ Y_2     │ Y_3     │ Y_4     │ Y_5     │ Y_6     │\n",
       "│     │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │\n",
       "├─────┼─────────┼─────────┼─────────┼─────────┼─────────┼─────────┼─────────┤\n",
       "│ 1   │ 0.0453  │ 0.0523  │ 0.0843  │ 0.0689  │ 0.1183  │ 0.2583  │ 0.2156  │\n",
       "│ 2   │ 0.0262  │ 0.0582  │ 0.1099  │ 0.1083  │ 0.0974  │ 0.228   │ 0.2431  │\n",
       "│ 3   │ 0.01    │ 0.0171  │ 0.0623  │ 0.0205  │ 0.0205  │ 0.0368  │ 0.1098  │\n",
       "│ 4   │ 0.0762  │ 0.0666  │ 0.0481  │ 0.0394  │ 0.059   │ 0.0649  │ 0.1209  │\n",
       "│ 5   │ 0.0286  │ 0.0453  │ 0.0277  │ 0.0174  │ 0.0384  │ 0.099   │ 0.1201  │\n",
       "│ 6   │ 0.0317  │ 0.0956  │ 0.1321  │ 0.1408  │ 0.1674  │ 0.171   │ 0.0731  │\n",
       "│ 7   │ 0.0519  │ 0.0548  │ 0.0842  │ 0.0319  │ 0.1158  │ 0.0922  │ 0.1027  │\n",
       "│ 8   │ 0.0223  │ 0.0375  │ 0.0484  │ 0.0475  │ 0.0647  │ 0.0591  │ 0.0753  │\n",
       "│ 9   │ 0.0164  │ 0.0173  │ 0.0347  │ 0.007   │ 0.0187  │ 0.0671  │ 0.1056  │\n",
       "│ 10  │ 0.0039  │ 0.0063  │ 0.0152  │ 0.0336  │ 0.031   │ 0.0284  │ 0.0396  │\n",
       "⋮\n",
       "│ 197 │ 0.0366  │ 0.0421  │ 0.0504  │ 0.025   │ 0.0596  │ 0.0252  │ 0.0958  │\n",
       "│ 198 │ 0.0238  │ 0.0318  │ 0.0422  │ 0.0399  │ 0.0788  │ 0.0766  │ 0.0881  │\n",
       "│ 199 │ 0.0116  │ 0.0744  │ 0.0367  │ 0.0225  │ 0.0076  │ 0.0545  │ 0.111   │\n",
       "│ 200 │ 0.0131  │ 0.0387  │ 0.0329  │ 0.0078  │ 0.0721  │ 0.1341  │ 0.1626  │\n",
       "│ 201 │ 0.0335  │ 0.0258  │ 0.0398  │ 0.057   │ 0.0529  │ 0.1091  │ 0.1709  │\n",
       "│ 202 │ 0.0272  │ 0.0378  │ 0.0488  │ 0.0848  │ 0.1127  │ 0.1103  │ 0.1349  │\n",
       "│ 203 │ 0.0187  │ 0.0346  │ 0.0168  │ 0.0177  │ 0.0393  │ 0.163   │ 0.2028  │\n",
       "│ 204 │ 0.0323  │ 0.0101  │ 0.0298  │ 0.0564  │ 0.076   │ 0.0958  │ 0.099   │\n",
       "│ 205 │ 0.0522  │ 0.0437  │ 0.018   │ 0.0292  │ 0.0351  │ 0.1171  │ 0.1257  │\n",
       "│ 206 │ 0.0303  │ 0.0353  │ 0.049   │ 0.0608  │ 0.0167  │ 0.1354  │ 0.1465  │\n",
       "│ 207 │ 0.026   │ 0.0363  │ 0.0136  │ 0.0272  │ 0.0214  │ 0.0338  │ 0.0655  │"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Rename the columns of the dataframe\n",
    "N = size(df, 2);\n",
    "Y = Array((1:N));\n",
    "rename!(df, fill(:Y, N), makeunique = true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fddf09",
   "metadata": {},
   "source": [
    "As mentioned before, the dataset has a class imbalance, we then verify this imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95e040d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{String, Int64} with 2 entries:\n",
       "  \"M\" => 111\n",
       "  \"R\" => 96"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countmap(df.Y_60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2319ff2f",
   "metadata": {},
   "source": [
    "In addition, for ease of data processing we are going to replace in the last column of the dataframe, thus, the \"R\" that designate the rocks will be 0 and the \"M\" that designate the mines will be 1. \n",
    "\n",
    "This procedure is known as like One Hot Encoding, which is the process of taking some categorical feature and transforming it into various binaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acf14f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Y_60 .= replace(df.Y_60, \"R\" => \"0\", \"M\" => \"1\");\n",
    "df.Y_60 = parse.(Float64, df.Y_60);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3e8379",
   "metadata": {},
   "source": [
    "There exists different ways to handle the Class Imbalance. A quite simple but popular strategy that works for data containers, is to either under- or over-sample it according to the class distribution. What that means is that the data container is re-sampled in such a way, that the class distribution in the resulting data container is approximately uniform.\n",
    "\n",
    "For this exercise we are going to work with oversampling, this approach generates a re-balanced version of data by repeatedly sampling existing observations in such a way that every class will have at least fraction times the number observations of the largest class.\n",
    "\n",
    "One consequence of oversampling is that it can overfit the model, let's see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5487d94c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>X</th><th>X_1</th><th>X_2</th><th>X_3</th><th>X_4</th><th>X_5</th><th>X_6</th><th>X_7</th><th>X_8</th></tr><tr><th></th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th></tr></thead><tbody><p>222 rows × 61 columns (omitted printing of 52 columns)</p><tr><th>1</th><td>0.0123</td><td>0.0022</td><td>0.0196</td><td>0.0206</td><td>0.018</td><td>0.0492</td><td>0.0033</td><td>0.0398</td><td>0.0791</td></tr><tr><th>2</th><td>0.0274</td><td>0.0242</td><td>0.0621</td><td>0.056</td><td>0.1129</td><td>0.0973</td><td>0.1823</td><td>0.1745</td><td>0.144</td></tr><tr><th>3</th><td>0.0762</td><td>0.0666</td><td>0.0481</td><td>0.0394</td><td>0.059</td><td>0.0649</td><td>0.1209</td><td>0.2467</td><td>0.3564</td></tr><tr><th>4</th><td>0.0335</td><td>0.0258</td><td>0.0398</td><td>0.057</td><td>0.0529</td><td>0.1091</td><td>0.1709</td><td>0.1684</td><td>0.1865</td></tr><tr><th>5</th><td>0.0201</td><td>0.0165</td><td>0.0344</td><td>0.033</td><td>0.0397</td><td>0.0443</td><td>0.0684</td><td>0.0903</td><td>0.1739</td></tr><tr><th>6</th><td>0.0599</td><td>0.0474</td><td>0.0498</td><td>0.0387</td><td>0.1026</td><td>0.0773</td><td>0.0853</td><td>0.0447</td><td>0.1094</td></tr><tr><th>7</th><td>0.0261</td><td>0.0266</td><td>0.0223</td><td>0.0749</td><td>0.1364</td><td>0.1513</td><td>0.1316</td><td>0.1654</td><td>0.1864</td></tr><tr><th>8</th><td>0.0107</td><td>0.0453</td><td>0.0289</td><td>0.0713</td><td>0.1075</td><td>0.1019</td><td>0.1606</td><td>0.2119</td><td>0.3061</td></tr><tr><th>9</th><td>0.013</td><td>0.0006</td><td>0.0088</td><td>0.0456</td><td>0.0525</td><td>0.0778</td><td>0.0931</td><td>0.0941</td><td>0.1711</td></tr><tr><th>10</th><td>0.0221</td><td>0.0065</td><td>0.0164</td><td>0.0487</td><td>0.0519</td><td>0.0849</td><td>0.0812</td><td>0.1833</td><td>0.2228</td></tr><tr><th>11</th><td>0.0071</td><td>0.0103</td><td>0.0135</td><td>0.0494</td><td>0.0253</td><td>0.0806</td><td>0.0701</td><td>0.0738</td><td>0.0117</td></tr><tr><th>12</th><td>0.01</td><td>0.0171</td><td>0.0623</td><td>0.0205</td><td>0.0205</td><td>0.0368</td><td>0.1098</td><td>0.1276</td><td>0.0598</td></tr><tr><th>13</th><td>0.0036</td><td>0.0078</td><td>0.0092</td><td>0.0387</td><td>0.053</td><td>0.1197</td><td>0.1243</td><td>0.1026</td><td>0.1239</td></tr><tr><th>14</th><td>0.0265</td><td>0.044</td><td>0.0137</td><td>0.0084</td><td>0.0305</td><td>0.0438</td><td>0.0341</td><td>0.078</td><td>0.0844</td></tr><tr><th>15</th><td>0.1088</td><td>0.1278</td><td>0.0926</td><td>0.1234</td><td>0.1276</td><td>0.1731</td><td>0.1948</td><td>0.4262</td><td>0.6828</td></tr><tr><th>16</th><td>0.0298</td><td>0.0615</td><td>0.065</td><td>0.0921</td><td>0.1615</td><td>0.2294</td><td>0.2176</td><td>0.2033</td><td>0.1459</td></tr><tr><th>17</th><td>0.0176</td><td>0.0172</td><td>0.0501</td><td>0.0285</td><td>0.0262</td><td>0.0351</td><td>0.0362</td><td>0.0535</td><td>0.0258</td></tr><tr><th>18</th><td>0.0093</td><td>0.0269</td><td>0.0217</td><td>0.0339</td><td>0.0305</td><td>0.1172</td><td>0.145</td><td>0.0638</td><td>0.074</td></tr><tr><th>19</th><td>0.0217</td><td>0.034</td><td>0.0392</td><td>0.0236</td><td>0.1081</td><td>0.1164</td><td>0.1398</td><td>0.1009</td><td>0.1147</td></tr><tr><th>20</th><td>0.0095</td><td>0.0308</td><td>0.0539</td><td>0.0411</td><td>0.0613</td><td>0.1039</td><td>0.1016</td><td>0.1394</td><td>0.2592</td></tr><tr><th>21</th><td>0.0216</td><td>0.0124</td><td>0.0174</td><td>0.0152</td><td>0.0608</td><td>0.1026</td><td>0.1139</td><td>0.0877</td><td>0.116</td></tr><tr><th>22</th><td>0.0209</td><td>0.0278</td><td>0.0115</td><td>0.0445</td><td>0.0427</td><td>0.0766</td><td>0.1458</td><td>0.143</td><td>0.1894</td></tr><tr><th>23</th><td>0.0516</td><td>0.0944</td><td>0.0622</td><td>0.0415</td><td>0.0995</td><td>0.2431</td><td>0.1777</td><td>0.2018</td><td>0.2611</td></tr><tr><th>24</th><td>0.0373</td><td>0.0281</td><td>0.0232</td><td>0.0225</td><td>0.0179</td><td>0.0733</td><td>0.0841</td><td>0.1031</td><td>0.0993</td></tr><tr><th>25</th><td>0.0126</td><td>0.0149</td><td>0.0641</td><td>0.1732</td><td>0.2565</td><td>0.2559</td><td>0.2947</td><td>0.411</td><td>0.4983</td></tr><tr><th>26</th><td>0.0253</td><td>0.0808</td><td>0.0507</td><td>0.0244</td><td>0.1724</td><td>0.3823</td><td>0.3729</td><td>0.3583</td><td>0.3429</td></tr><tr><th>27</th><td>0.0303</td><td>0.0353</td><td>0.049</td><td>0.0608</td><td>0.0167</td><td>0.1354</td><td>0.1465</td><td>0.1123</td><td>0.1945</td></tr><tr><th>28</th><td>0.0442</td><td>0.0477</td><td>0.0049</td><td>0.0581</td><td>0.0278</td><td>0.0678</td><td>0.1664</td><td>0.149</td><td>0.0974</td></tr><tr><th>29</th><td>0.0368</td><td>0.0403</td><td>0.0317</td><td>0.0293</td><td>0.082</td><td>0.1342</td><td>0.1161</td><td>0.0663</td><td>0.0155</td></tr><tr><th>30</th><td>0.0177</td><td>0.03</td><td>0.0288</td><td>0.0394</td><td>0.063</td><td>0.0526</td><td>0.0688</td><td>0.0633</td><td>0.0624</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccccccc}\n",
       "\t& X & X\\_1 & X\\_2 & X\\_3 & X\\_4 & X\\_5 & X\\_6 & X\\_7 & X\\_8 & \\\\\n",
       "\t\\hline\n",
       "\t& Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & \\\\\n",
       "\t\\hline\n",
       "\t1 & 0.0123 & 0.0022 & 0.0196 & 0.0206 & 0.018 & 0.0492 & 0.0033 & 0.0398 & 0.0791 & $\\dots$ \\\\\n",
       "\t2 & 0.0274 & 0.0242 & 0.0621 & 0.056 & 0.1129 & 0.0973 & 0.1823 & 0.1745 & 0.144 & $\\dots$ \\\\\n",
       "\t3 & 0.0762 & 0.0666 & 0.0481 & 0.0394 & 0.059 & 0.0649 & 0.1209 & 0.2467 & 0.3564 & $\\dots$ \\\\\n",
       "\t4 & 0.0335 & 0.0258 & 0.0398 & 0.057 & 0.0529 & 0.1091 & 0.1709 & 0.1684 & 0.1865 & $\\dots$ \\\\\n",
       "\t5 & 0.0201 & 0.0165 & 0.0344 & 0.033 & 0.0397 & 0.0443 & 0.0684 & 0.0903 & 0.1739 & $\\dots$ \\\\\n",
       "\t6 & 0.0599 & 0.0474 & 0.0498 & 0.0387 & 0.1026 & 0.0773 & 0.0853 & 0.0447 & 0.1094 & $\\dots$ \\\\\n",
       "\t7 & 0.0261 & 0.0266 & 0.0223 & 0.0749 & 0.1364 & 0.1513 & 0.1316 & 0.1654 & 0.1864 & $\\dots$ \\\\\n",
       "\t8 & 0.0107 & 0.0453 & 0.0289 & 0.0713 & 0.1075 & 0.1019 & 0.1606 & 0.2119 & 0.3061 & $\\dots$ \\\\\n",
       "\t9 & 0.013 & 0.0006 & 0.0088 & 0.0456 & 0.0525 & 0.0778 & 0.0931 & 0.0941 & 0.1711 & $\\dots$ \\\\\n",
       "\t10 & 0.0221 & 0.0065 & 0.0164 & 0.0487 & 0.0519 & 0.0849 & 0.0812 & 0.1833 & 0.2228 & $\\dots$ \\\\\n",
       "\t11 & 0.0071 & 0.0103 & 0.0135 & 0.0494 & 0.0253 & 0.0806 & 0.0701 & 0.0738 & 0.0117 & $\\dots$ \\\\\n",
       "\t12 & 0.01 & 0.0171 & 0.0623 & 0.0205 & 0.0205 & 0.0368 & 0.1098 & 0.1276 & 0.0598 & $\\dots$ \\\\\n",
       "\t13 & 0.0036 & 0.0078 & 0.0092 & 0.0387 & 0.053 & 0.1197 & 0.1243 & 0.1026 & 0.1239 & $\\dots$ \\\\\n",
       "\t14 & 0.0265 & 0.044 & 0.0137 & 0.0084 & 0.0305 & 0.0438 & 0.0341 & 0.078 & 0.0844 & $\\dots$ \\\\\n",
       "\t15 & 0.1088 & 0.1278 & 0.0926 & 0.1234 & 0.1276 & 0.1731 & 0.1948 & 0.4262 & 0.6828 & $\\dots$ \\\\\n",
       "\t16 & 0.0298 & 0.0615 & 0.065 & 0.0921 & 0.1615 & 0.2294 & 0.2176 & 0.2033 & 0.1459 & $\\dots$ \\\\\n",
       "\t17 & 0.0176 & 0.0172 & 0.0501 & 0.0285 & 0.0262 & 0.0351 & 0.0362 & 0.0535 & 0.0258 & $\\dots$ \\\\\n",
       "\t18 & 0.0093 & 0.0269 & 0.0217 & 0.0339 & 0.0305 & 0.1172 & 0.145 & 0.0638 & 0.074 & $\\dots$ \\\\\n",
       "\t19 & 0.0217 & 0.034 & 0.0392 & 0.0236 & 0.1081 & 0.1164 & 0.1398 & 0.1009 & 0.1147 & $\\dots$ \\\\\n",
       "\t20 & 0.0095 & 0.0308 & 0.0539 & 0.0411 & 0.0613 & 0.1039 & 0.1016 & 0.1394 & 0.2592 & $\\dots$ \\\\\n",
       "\t21 & 0.0216 & 0.0124 & 0.0174 & 0.0152 & 0.0608 & 0.1026 & 0.1139 & 0.0877 & 0.116 & $\\dots$ \\\\\n",
       "\t22 & 0.0209 & 0.0278 & 0.0115 & 0.0445 & 0.0427 & 0.0766 & 0.1458 & 0.143 & 0.1894 & $\\dots$ \\\\\n",
       "\t23 & 0.0516 & 0.0944 & 0.0622 & 0.0415 & 0.0995 & 0.2431 & 0.1777 & 0.2018 & 0.2611 & $\\dots$ \\\\\n",
       "\t24 & 0.0373 & 0.0281 & 0.0232 & 0.0225 & 0.0179 & 0.0733 & 0.0841 & 0.1031 & 0.0993 & $\\dots$ \\\\\n",
       "\t25 & 0.0126 & 0.0149 & 0.0641 & 0.1732 & 0.2565 & 0.2559 & 0.2947 & 0.411 & 0.4983 & $\\dots$ \\\\\n",
       "\t26 & 0.0253 & 0.0808 & 0.0507 & 0.0244 & 0.1724 & 0.3823 & 0.3729 & 0.3583 & 0.3429 & $\\dots$ \\\\\n",
       "\t27 & 0.0303 & 0.0353 & 0.049 & 0.0608 & 0.0167 & 0.1354 & 0.1465 & 0.1123 & 0.1945 & $\\dots$ \\\\\n",
       "\t28 & 0.0442 & 0.0477 & 0.0049 & 0.0581 & 0.0278 & 0.0678 & 0.1664 & 0.149 & 0.0974 & $\\dots$ \\\\\n",
       "\t29 & 0.0368 & 0.0403 & 0.0317 & 0.0293 & 0.082 & 0.1342 & 0.1161 & 0.0663 & 0.0155 & $\\dots$ \\\\\n",
       "\t30 & 0.0177 & 0.03 & 0.0288 & 0.0394 & 0.063 & 0.0526 & 0.0688 & 0.0633 & 0.0624 & $\\dots$ \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ &  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "222×61 DataFrame. Omitted printing of 54 columns\n",
       "│ Row │ X       │ X_1     │ X_2     │ X_3     │ X_4     │ X_5     │ X_6     │\n",
       "│     │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │\n",
       "├─────┼─────────┼─────────┼─────────┼─────────┼─────────┼─────────┼─────────┤\n",
       "│ 1   │ 0.0123  │ 0.0022  │ 0.0196  │ 0.0206  │ 0.018   │ 0.0492  │ 0.0033  │\n",
       "│ 2   │ 0.0274  │ 0.0242  │ 0.0621  │ 0.056   │ 0.1129  │ 0.0973  │ 0.1823  │\n",
       "│ 3   │ 0.0762  │ 0.0666  │ 0.0481  │ 0.0394  │ 0.059   │ 0.0649  │ 0.1209  │\n",
       "│ 4   │ 0.0335  │ 0.0258  │ 0.0398  │ 0.057   │ 0.0529  │ 0.1091  │ 0.1709  │\n",
       "│ 5   │ 0.0201  │ 0.0165  │ 0.0344  │ 0.033   │ 0.0397  │ 0.0443  │ 0.0684  │\n",
       "│ 6   │ 0.0599  │ 0.0474  │ 0.0498  │ 0.0387  │ 0.1026  │ 0.0773  │ 0.0853  │\n",
       "│ 7   │ 0.0261  │ 0.0266  │ 0.0223  │ 0.0749  │ 0.1364  │ 0.1513  │ 0.1316  │\n",
       "│ 8   │ 0.0107  │ 0.0453  │ 0.0289  │ 0.0713  │ 0.1075  │ 0.1019  │ 0.1606  │\n",
       "│ 9   │ 0.013   │ 0.0006  │ 0.0088  │ 0.0456  │ 0.0525  │ 0.0778  │ 0.0931  │\n",
       "│ 10  │ 0.0221  │ 0.0065  │ 0.0164  │ 0.0487  │ 0.0519  │ 0.0849  │ 0.0812  │\n",
       "⋮\n",
       "│ 212 │ 0.0231  │ 0.0351  │ 0.003   │ 0.0304  │ 0.0339  │ 0.086   │ 0.1738  │\n",
       "│ 213 │ 0.0731  │ 0.1249  │ 0.1665  │ 0.1496  │ 0.1443  │ 0.277   │ 0.2555  │\n",
       "│ 214 │ 0.0195  │ 0.0213  │ 0.0058  │ 0.019   │ 0.0319  │ 0.0571  │ 0.1004  │\n",
       "│ 215 │ 0.0307  │ 0.0523  │ 0.0653  │ 0.0521  │ 0.0611  │ 0.0577  │ 0.0665  │\n",
       "│ 216 │ 0.0293  │ 0.0378  │ 0.0257  │ 0.0062  │ 0.013   │ 0.0612  │ 0.0895  │\n",
       "│ 217 │ 0.0132  │ 0.008   │ 0.0188  │ 0.0141  │ 0.0436  │ 0.0668  │ 0.0609  │\n",
       "│ 218 │ 0.0131  │ 0.0201  │ 0.0045  │ 0.0217  │ 0.023   │ 0.0481  │ 0.0742  │\n",
       "│ 219 │ 0.0378  │ 0.0318  │ 0.0423  │ 0.035   │ 0.1787  │ 0.1635  │ 0.0887  │\n",
       "│ 220 │ 0.0233  │ 0.0394  │ 0.0416  │ 0.0547  │ 0.0993  │ 0.1515  │ 0.1674  │\n",
       "│ 221 │ 0.0968  │ 0.0821  │ 0.0629  │ 0.0608  │ 0.0617  │ 0.1207  │ 0.0944  │\n",
       "│ 222 │ 0.0039  │ 0.0063  │ 0.0152  │ 0.0336  │ 0.031   │ 0.0284  │ 0.0396  │"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Handling Class Imbalance with oversampling, the data is organized and the new balanced dataframe is generated.\n",
    "using MLDataPattern\n",
    "A = Array(df[:, 1:60]);\n",
    "b = df.Y_60;\n",
    "A_bal, b_bal = oversample((A', b));\n",
    "df2 = [A_bal' b_bal];\n",
    "df3 = DataFrame(df2)\n",
    "M = size(df3, 2);\n",
    "X = Array((1:M));\n",
    "rename!(df3, fill(:X, M), makeunique = true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fe3791b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(155×61 DataFrame. Omitted printing of 54 columns\n",
       "│ Row │ X       │ X_1     │ X_2     │ X_3     │ X_4     │ X_5     │ X_6     │\n",
       "│     │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │\n",
       "├─────┼─────────┼─────────┼─────────┼─────────┼─────────┼─────────┼─────────┤\n",
       "│ 1   │ 0.0123  │ 0.0022  │ 0.0196  │ 0.0206  │ 0.018   │ 0.0492  │ 0.0033  │\n",
       "│ 2   │ 0.0274  │ 0.0242  │ 0.0621  │ 0.056   │ 0.1129  │ 0.0973  │ 0.1823  │\n",
       "│ 3   │ 0.0762  │ 0.0666  │ 0.0481  │ 0.0394  │ 0.059   │ 0.0649  │ 0.1209  │\n",
       "│ 4   │ 0.0335  │ 0.0258  │ 0.0398  │ 0.057   │ 0.0529  │ 0.1091  │ 0.1709  │\n",
       "│ 5   │ 0.0599  │ 0.0474  │ 0.0498  │ 0.0387  │ 0.1026  │ 0.0773  │ 0.0853  │\n",
       "│ 6   │ 0.013   │ 0.0006  │ 0.0088  │ 0.0456  │ 0.0525  │ 0.0778  │ 0.0931  │\n",
       "│ 7   │ 0.0221  │ 0.0065  │ 0.0164  │ 0.0487  │ 0.0519  │ 0.0849  │ 0.0812  │\n",
       "│ 8   │ 0.0071  │ 0.0103  │ 0.0135  │ 0.0494  │ 0.0253  │ 0.0806  │ 0.0701  │\n",
       "│ 9   │ 0.01    │ 0.0171  │ 0.0623  │ 0.0205  │ 0.0205  │ 0.0368  │ 0.1098  │\n",
       "│ 10  │ 0.0036  │ 0.0078  │ 0.0092  │ 0.0387  │ 0.053   │ 0.1197  │ 0.1243  │\n",
       "⋮\n",
       "│ 145 │ 0.0473  │ 0.0509  │ 0.0819  │ 0.1252  │ 0.1783  │ 0.307   │ 0.3008  │\n",
       "│ 146 │ 0.0135  │ 0.0045  │ 0.0051  │ 0.0289  │ 0.0561  │ 0.0929  │ 0.1031  │\n",
       "│ 147 │ 0.0071  │ 0.0103  │ 0.0135  │ 0.0494  │ 0.0253  │ 0.0806  │ 0.0701  │\n",
       "│ 148 │ 0.0231  │ 0.0351  │ 0.003   │ 0.0304  │ 0.0339  │ 0.086   │ 0.1738  │\n",
       "│ 149 │ 0.0307  │ 0.0523  │ 0.0653  │ 0.0521  │ 0.0611  │ 0.0577  │ 0.0665  │\n",
       "│ 150 │ 0.0293  │ 0.0378  │ 0.0257  │ 0.0062  │ 0.013   │ 0.0612  │ 0.0895  │\n",
       "│ 151 │ 0.0132  │ 0.008   │ 0.0188  │ 0.0141  │ 0.0436  │ 0.0668  │ 0.0609  │\n",
       "│ 152 │ 0.0378  │ 0.0318  │ 0.0423  │ 0.035   │ 0.1787  │ 0.1635  │ 0.0887  │\n",
       "│ 153 │ 0.0233  │ 0.0394  │ 0.0416  │ 0.0547  │ 0.0993  │ 0.1515  │ 0.1674  │\n",
       "│ 154 │ 0.0968  │ 0.0821  │ 0.0629  │ 0.0608  │ 0.0617  │ 0.1207  │ 0.0944  │\n",
       "│ 155 │ 0.0039  │ 0.0063  │ 0.0152  │ 0.0336  │ 0.031   │ 0.0284  │ 0.0396  │, 67×61 DataFrame. Omitted printing of 54 columns\n",
       "│ Row │ X       │ X_1     │ X_2     │ X_3     │ X_4     │ X_5     │ X_6     │\n",
       "│     │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │\n",
       "├─────┼─────────┼─────────┼─────────┼─────────┼─────────┼─────────┼─────────┤\n",
       "│ 1   │ 0.0201  │ 0.0165  │ 0.0344  │ 0.033   │ 0.0397  │ 0.0443  │ 0.0684  │\n",
       "│ 2   │ 0.0261  │ 0.0266  │ 0.0223  │ 0.0749  │ 0.1364  │ 0.1513  │ 0.1316  │\n",
       "│ 3   │ 0.0107  │ 0.0453  │ 0.0289  │ 0.0713  │ 0.1075  │ 0.1019  │ 0.1606  │\n",
       "│ 4   │ 0.0373  │ 0.0281  │ 0.0232  │ 0.0225  │ 0.0179  │ 0.0733  │ 0.0841  │\n",
       "│ 5   │ 0.0253  │ 0.0808  │ 0.0507  │ 0.0244  │ 0.1724  │ 0.3823  │ 0.3729  │\n",
       "│ 6   │ 0.0442  │ 0.0477  │ 0.0049  │ 0.0581  │ 0.0278  │ 0.0678  │ 0.1664  │\n",
       "│ 7   │ 0.0368  │ 0.0403  │ 0.0317  │ 0.0293  │ 0.082   │ 0.1342  │ 0.1161  │\n",
       "│ 8   │ 0.0394  │ 0.042   │ 0.0446  │ 0.0551  │ 0.0597  │ 0.1416  │ 0.0956  │\n",
       "│ 9   │ 0.0094  │ 0.0166  │ 0.0398  │ 0.0359  │ 0.0681  │ 0.0706  │ 0.102   │\n",
       "│ 10  │ 0.0257  │ 0.0447  │ 0.0388  │ 0.0239  │ 0.1315  │ 0.1323  │ 0.1608  │\n",
       "⋮\n",
       "│ 57  │ 0.0423  │ 0.0321  │ 0.0709  │ 0.0108  │ 0.107   │ 0.0973  │ 0.0961  │\n",
       "│ 58  │ 0.0216  │ 0.0215  │ 0.0273  │ 0.0139  │ 0.0357  │ 0.0785  │ 0.0906  │\n",
       "│ 59  │ 0.0201  │ 0.0178  │ 0.0274  │ 0.0232  │ 0.0724  │ 0.0833  │ 0.1232  │\n",
       "│ 60  │ 0.026   │ 0.0192  │ 0.0254  │ 0.0061  │ 0.0352  │ 0.0701  │ 0.1263  │\n",
       "│ 61  │ 0.0459  │ 0.0437  │ 0.0347  │ 0.0456  │ 0.0067  │ 0.089   │ 0.1798  │\n",
       "│ 62  │ 0.0115  │ 0.015   │ 0.0136  │ 0.0076  │ 0.0211  │ 0.1058  │ 0.1023  │\n",
       "│ 63  │ 0.0408  │ 0.0653  │ 0.0397  │ 0.0604  │ 0.0496  │ 0.1817  │ 0.1178  │\n",
       "│ 64  │ 0.0269  │ 0.0383  │ 0.0505  │ 0.0707  │ 0.1313  │ 0.2103  │ 0.2263  │\n",
       "│ 65  │ 0.0731  │ 0.1249  │ 0.1665  │ 0.1496  │ 0.1443  │ 0.277   │ 0.2555  │\n",
       "│ 66  │ 0.0195  │ 0.0213  │ 0.0058  │ 0.019   │ 0.0319  │ 0.0571  │ 0.1004  │\n",
       "│ 67  │ 0.0131  │ 0.0201  │ 0.0045  │ 0.0217  │ 0.023   │ 0.0481  │ 0.0742  │)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Split the Train and Test Data\n",
    "using Random\n",
    "Random.seed!(1)\n",
    "using Lathe.preprocess: TrainTestSplit\n",
    "train, test = TrainTestSplit(df3,.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1071fa56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples used for training:155\n",
      "Examples used for testing:67\n"
     ]
    }
   ],
   "source": [
    "#Size of the train and test data\n",
    "println(\"Examples used for training:\", size(train,1))\n",
    "println(\"Examples used for testing:\", size(test,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f537eab",
   "metadata": {},
   "source": [
    "As we had mentioned initially, to work the binary classification problem, we are going to use different algorithms. Before working with each algorithm, it is important to choose a performance measure that will allow us, as its name indicates, to measure how well an algorithm works.\n",
    "\n",
    "In our case we will choose the accuracy. Accuracy is a useful metric for evaluating the performance of classification models in machine learning.\n",
    "\n",
    "This measure allows us to know, on average, how well the data are classified when the errors in the prediction of the classes are equally important.The accuracy is given by:\n",
    "\n",
    "\\begin{equation*}\n",
    "Accuracy = \\frac{TP+TN}{TP+TN+FP+FN}\n",
    "\\end{equation*}\n",
    "\n",
    "where $TP = $ True Positives, $TN= $ True Negatives, $FP=$ False Positives and $FN = $ False Negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fde87b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_train = Array(train[:, 1:60]);\n",
    "b_train = train.X_60;\n",
    "A_test = Array(test[:, 1:60]);\n",
    "b_test = test.X_60;\n",
    "\n",
    "#transpose data\n",
    "A_train_t = A_train';\n",
    "A_test_t = A_test';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3fd9ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the model is: 0.7761194029850746\n"
     ]
    }
   ],
   "source": [
    "#Linear Regresion\n",
    "using GLM\n",
    "\n",
    "fm = @formula(X_60 ~ X+X_2+X_3+X_4+X_5+X_6+X_7+X_8+X_9+X_10+X_11+X_12+X_13+X_14+X_15+X_16+X_17+X_18+X_19+X_20+X_21+X_22+X_23+X_24+X_25+X_26+X_27+X_28+X_29+X_30+X_31+X_32+X_33+X_34+X_35+X_36+X_37+X_38+X_39+X_40+X_41+X_42+X_43+X_44+X_45+X_46+X_47+X_48+X_49+X_50+X_51+X_52+X_53+X_54+X_55+X_56+X_57+X_58+X_59);\n",
    "linearRegressor = lm(fm, train);\n",
    "prediction1 = GLM.predict(linearRegressor, test);\n",
    "\n",
    "#Classification\n",
    "prediction_class1 = [if x < 0.5 0.0 else 1.0 end for x in prediction1];\n",
    "prediction1 =  prediction_class1;\n",
    "\n",
    "#Accuracy calculation\n",
    "accuracy1 = mean(prediction1 .== b_test);\n",
    "println(\"The accuracy of the model is: \", accuracy1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "132a2d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the model is: 0.7164179104477612\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regresion\n",
    "logit = glm(fm, train, Binomial(), ProbitLink())\n",
    "prediction2 = GLM.predict(logit, test)\n",
    "\n",
    "#Classification\n",
    "prediction_class2 = [if x < 0.5 0.0 else 1.0 end for x in prediction2];\n",
    "prediction2 = prediction_class2\n",
    "\n",
    "#Accuracy calculation\n",
    "accuracy2 = mean(prediction2 .== b_test);\n",
    "println(\"The accuracy of the model is: \", accuracy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "299defd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the model is: 0.6268656716417911\n"
     ]
    }
   ],
   "source": [
    "#SVM\n",
    "using LIBSVM\n",
    "\n",
    "#run model\n",
    "svmmodel = svmtrain(A_train_t, b_train);\n",
    "prediction3, decision_values = svmpredict(svmmodel, A_test_t);\n",
    "\n",
    "#Accuracy calculation\n",
    "accuracy3 = mean(prediction3 .== b_test);\n",
    "println(\"The accuracy of the model is: \", accuracy3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b976f653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the model is: 0.7014925373134329\n"
     ]
    }
   ],
   "source": [
    "#KNN\n",
    "using NearestNeighbors\n",
    "\n",
    "kdtree = KDTree(A_train_t);\n",
    "\n",
    "#run model\n",
    "k = 5;\n",
    "idxs, dists = knn(kdtree, A_test_t, k, true);\n",
    "\n",
    "#post-proccess\n",
    "idxs_matrix = hcat(idxs...);\n",
    "idxs_matrix_t = idxs_matrix';\n",
    "knn_class = b_train[idxs_matrix_t];\n",
    "df_knn_class = DataFrame(knn_class);\n",
    "\n",
    "#Make Predictions\n",
    "prediction4 = [];\n",
    "for i = 1:size(df_knn_class,1)\n",
    "    pred = argmax(countmap(df_knn_class[i, :]))\n",
    "    push!(prediction4, pred)\n",
    "end\n",
    "\n",
    "#Accuracy calculation\n",
    "accuracy4 = mean(prediction4 .== b_test);\n",
    "println(\"The accuracy of the model is: \", accuracy4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36ac766c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the model is: 0.7014925373134329\n"
     ]
    }
   ],
   "source": [
    "#Decision Tree\n",
    "using DecisionTree\n",
    "dtmodel = DecisionTreeClassifier()\n",
    "DecisionTree.fit!(dtmodel, A_train, b_train)\n",
    "\n",
    "#Make predictions\n",
    "prediction5 = DecisionTree.predict(dtmodel, A_test)\n",
    "\n",
    "#Accuracy calculation\n",
    "accuracy5 = mean(prediction5 .== b_test);\n",
    "println(\"The accuracy of the model is: \", accuracy5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9b22de",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3193dbe5",
   "metadata": {},
   "source": [
    "According to the results obtained in the implementation of the algorithms and considering the calculation of the performance measure, the algorithm with the highest performance in the binary classification problem is Linear Regression with a $accuracy = 77.61194029850746 \\% $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cfe622",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.2",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
